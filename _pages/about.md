---
layout: archive
title: "About Myself"
# permalink: /about/
permalink: /
author_profile: true
redirect_from:
  - /about/
  - /about.html
  # - /
---

I am currently a third-year Ph.D. candidate in the <strong>Computer Vision and Robotic Perception (CVRP)</strong> Laboratory at <strong> National University of Singapore</strong>, supervised by Prof. [Gim Hee Lee](https://www.comp.nus.edu.sg/~leegh/). Before that, I received my M.Comp. degree from School of Computing, National University of Singapore, and B.Eng. degree from Department of Automation in Tsinghua University, advised by Prof. [Jiwen Lu]([https://scholar.google.com/citations?user=WH0J_34AAAAJ&hl=en](http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/)). I'm broadly interested in 3D Vision, especially in 3D Gaussian Splatting, Neural Radiance Fields, and 3D Scene Understanding, where I focused on enhancing the efficency and generalization ability of models.


News
======
<style>
  .news-container p {
    margin: 5px 0; /* Ë∞ÉÊï¥ÊÆµËêΩÈó¥Ë∑ù */
    line-height: 1.2; /* Ë∞ÉÊï¥Ë°åÈ´ò */
  }

  .show-more-link {
    text-align: center;
    display: block;
    margin-top: 10px;
  }
</style>

<div class="news-container">
  <p>üöÄ <strong>[09.2024]</strong> Two papers <a href="https://arxiv.org/pdf/2405.17958">FreeSplat</a> and <a href="https://hlinchen.github.io/projects/VCR-GauS/">VCR-GauS</a> were accepted at <span style="color: red;">NeurIPS 2024</span>!</p>
  <p>üòé <strong>[07.2024]</strong> Our paper OHDA is accepted at <span style="color: red;">BMVC 2024</span>!</p>
  <p>üôá <strong>[02.2024]</strong> Our paper <a href="https://arxiv.org/pdf/2404.00931">GOV-NeSF</a> is accepted at <span style="color: red;">CVPR 2024</span>!</p>
  <p>üòé <strong>[11.2023]</strong> One paper is accepted at <span style="color: red;">WACV 2024</span> as an oral paper!</p>
  <p>üòé <strong>[10.2023]</strong> Our paper GRL is accepted at <span style="color: red;">3DV 2024</span> as an oral paper!</p>
  <p>üöÄ <strong>[08.2022]</strong> I am excited to join the <a href="https://www.comp.nus.edu.sg/~leegh/">CVRP lab</a> of SoC, NUS as a PhD student!</p>
</div>

<br>

Preprints
======
<div style="display: flex; align-items: center; margin-bottom: 40px;">
  <img src="images/free2.gif" alt="FreeSplat: Generalizable 3D Gaussian Splatting Towards Free-View Synthesis of Indoor Scenes" style="width: 350px; height: auto; margin-right: 20px;">
  <div>
    <h3 style="margin: 0;">FreeSplat++: Generalizable 3D Gaussian Splatting for Efficient Indoor Scene Reconstruction</h3>
    <p style="margin: 5px 0;">
          <strong>Yunsong Wang</strong>,
          <a href="https://tianxinhuang.github.io/">Tianxin Huang</a>,
          <a href="https://hlinchen.github.io/">Hanlin Chen</a>,
          <a href="https://www.comp.nus.edu.sg/~leegh/">Gim Hee Lee</a>
          <br>
      <b><em>Arxiv 2025</em></b><br>
      <!-- color: #0073e6; -->
      <a href="https://wangys16.github.io/FreeSplatPP-Page/" style="text-decoration: none; ">[Project Page]</a>
      <a href="https://arxiv.org/pdf/2503.22986" style="text-decoration: none;">[PDF]</a> 
      <a href="https://github.com/wangys16/FreeSplatPP" style="text-decoration: none;">[Code]</a>
      <!-- <a href="https://video.com" style="text-decoration: none;">Video</a> / -->
      <!-- <a class="more-link" href="https://github.com/HeliosZhao/Animate124" target="_blank"><img alt="GitHub stars" align="right"
        src="https://img.shields.io/github/stars/HeliosZhao/Animate124?style=social"></a> -->
    </p>
    <!-- <p style="margin: 5px 0;">
      The first work to animate a single in-the-wild image into 3D video through textual motion descriptions.
    </p> -->
    <div style="display: flex; align-items: center; margin-top: 10px;">
      <a href="https://github.com/yourrepo" style="display: flex; align-items: center; text-decoration: none; color: #000;">
      </a>
    </div>
  </div>
</div>
<div style="display: flex; align-items: center; margin-bottom: 40px;">
  <img src="images/NeuSG.jpg" alt="NeuSG: Neural implicit surface reconstruction with 3d gaussian splatting guidance" style="width: 350px; height: auto; margin-right: 20px;">

  <div>
    <h3 style="margin: 0;">NeuSG: Neural implicit surface reconstruction with 3d gaussian splatting guidance</h3>
    <p style="margin: 5px 0;">
          <a href="https://hlinchen.github.io/">Hanlin Chen</a>,
          <a href="https://chaneyddtt.github.io/">Chen Li</a>,
          <strong>Yunsong Wang</strong>,
          <a href="https://www.comp.nus.edu.sg/~leegh/">Gim Hee Lee</a>
          <br>
      <b><em>arXiv 2023</em></b><br>
      <!-- <a href="https://projectpage.com" style="text-decoration: none;">[Project Page]</a> -->
      <a href="https://arxiv.org/pdf/2312.00846" style="text-decoration: none;">[PDF]</a>
      <!-- <a href="https://github.com/wangys16/GOV-NeSF" style="text-decoration: none;">[Code]</a> -->
      <!-- <a href="https://video.com" style="text-decoration: none;">[Video]</a> -->
      <!-- <a class="more-link" href="https://github.com/HeliosZhao/Animate124" target="_blank"><img alt="GitHub stars" align="right"
        src="https://img.shields.io/github/stars/HeliosZhao/Animate124?style=social"></a> -->
    </p>
    <!-- <p style="margin: 5px 0;">
      The first work to animate a single in-the-wild image into 3D video through textual motion descriptions.
    </p> -->
    <div style="display: flex; align-items: center; margin-top: 10px;">
      <a href="https://github.com/yourrepo" style="display: flex; align-items: center; text-decoration: none; color: #000;">
      </a>
    </div>
  </div>
</div>

Publications
======
<div style="display: flex; align-items: center; margin-bottom: 40px;">
  <img src="images/FreeSplat.jpg" alt="FreeSplat: Generalizable 3D Gaussian Splatting Towards Free-View Synthesis of Indoor Scenes" style="width: 350px; height: auto; margin-right: 20px;">
  <div>
    <h3 style="margin: 0;">FreeSplat: Generalizable 3D Gaussian Splatting Towards Free-View Synthesis of Indoor Scenes</h3>
    <p style="margin: 5px 0;">
          <strong>Yunsong Wang</strong>,
          <a href="https://tianxinhuang.github.io/">Tianxin Huang</a>,
          <a href="https://hlinchen.github.io/">Hanlin Chen</a>,
          <a href="https://www.comp.nus.edu.sg/~leegh/">Gim Hee Lee</a>
          <br>
      <b><em>NeurIPS 2024</em></b><br>
      <!-- color: #0073e6; -->
      <a href="https://wangys16.github.io/FreeSplat-project/" style="text-decoration: none; ">[Project Page]</a>
      <a href="https://arxiv.org/pdf/2405.17958" style="text-decoration: none;">[PDF]</a> 
      <a href="https://github.com/wangys16/FreeSplat" style="text-decoration: none;">[Code]</a>
      <!-- <a href="https://video.com" style="text-decoration: none;">Video</a> / -->
      <!-- <a class="more-link" href="https://github.com/HeliosZhao/Animate124" target="_blank"><img alt="GitHub stars" align="right"
        src="https://img.shields.io/github/stars/HeliosZhao/Animate124?style=social"></a> -->
    </p>
    <!-- <p style="margin: 5px 0;">
      The first work to animate a single in-the-wild image into 3D video through textual motion descriptions.
    </p> -->
    <div style="display: flex; align-items: center; margin-top: 10px;">
      <a href="https://github.com/yourrepo" style="display: flex; align-items: center; text-decoration: none; color: #000;">
      </a>
    </div>
  </div>
</div>

<div style="display: flex; align-items: center; margin-bottom: 40px;">
  <img src="images/VCR-GauS.jpg" alt="VCR-GauS: View Consistent Depth-Normal Regularizer for Gaussian Surface Reconstruction" style="width: 350px; height: auto; margin-right: 20px;">
  <div>
    <h3 style="margin: 0;">VCR-GauS: View Consistent Depth-Normal Regularizer for Gaussian Surface Reconstruction</h3>
    <p style="margin: 5px 0;">
          <a href="https://hlinchen.github.io/">Hanlin Chen</a>,
          <a href="https://weify627.github.io/">Fangyin Wei</a>,
          <a href="https://chaneyddtt.github.io/">Chen Li</a>,
          <a href="https://tianxinhuang.github.io/">Tianxin Huang</a>,
          <strong>Yunsong Wang</strong>,
          <a href="https://www.comp.nus.edu.sg/~leegh/">Gim Hee Lee</a>
          <br>
      <b><em>NeurIPS 2024</em></b><br>
      <!-- color: #0073e6; -->
      <a href="https://hlinchen.github.io/projects/VCR-GauS/" style="text-decoration: none; ">[Project Page]</a>
      <a href="https://arxiv.org/pdf/2406.05774" style="text-decoration: none;">[PDF]</a> 
      <a href="https://github.com/HLinChen/VCR-GauS" style="text-decoration: none;">[Code]</a>
      <!-- <a href="https://video.com" style="text-decoration: none;">Video</a> / -->
      <!-- <a class="more-link" href="https://github.com/HeliosZhao/Animate124" target="_blank"><img alt="GitHub stars" align="right"
        src="https://img.shields.io/github/stars/HeliosZhao/Animate124?style=social"></a> -->
    </p>
    <!-- <p style="margin: 5px 0;">
      The first work to animate a single in-the-wild image into 3D video through textual motion descriptions.
    </p> -->
    <div style="display: flex; align-items: center; margin-top: 10px;">
      <a href="https://github.com/yourrepo" style="display: flex; align-items: center; text-decoration: none; color: #000;">
      </a>
    </div>
  </div>
</div>

<div style="display: flex; align-items: center; margin-bottom: 40px;">
  <img src="images/OHDA.jpg" alt="Syn-to-Real Unsupervised Domain Adaptation for Indoor 3D Object Detection" style="width: 350px; height: auto; margin-right: 20px;">

  <div>
    <h3 style="margin: 0;">Syn-to-Real Unsupervised Domain Adaptation for Indoor 3D Object Detection</h3>
    <p style="margin: 5px 0;">
          <strong>Yunsong Wang</strong>,
          <a href="https://na-z.github.io">Na Zhao</a>,
          <a href="https://www.comp.nus.edu.sg/~leegh/">Gim Hee Lee</a>
          <br>
      <b><em>BMVC 2024</em></b><br>
      <!-- color: #0073e6; -->
      <!-- <a href="https://projectpage.com" style="text-decoration: none; ">Project Page</a> / -->
      <a href="https://arxiv.org/pdf/2406.11311" style="text-decoration: none;">[PDF]</a> 
<!--       <a href="https://github.com/wangys16/GOV-NeSF" style="text-decoration: none;">[Code]</a> -->
      <!-- <a href="https://video.com" style="text-decoration: none;">Video</a> / -->
      <!-- <a class="more-link" href="https://github.com/HeliosZhao/Animate124" target="_blank"><img alt="GitHub stars" align="right"
        src="https://img.shields.io/github/stars/HeliosZhao/Animate124?style=social"></a> -->
    </p>
    <!-- <p style="margin: 5px 0;">
      The first work to animate a single in-the-wild image into 3D video through textual motion descriptions.
    </p> -->
    <div style="display: flex; align-items: center; margin-top: 10px;">
      <a href="https://github.com/yourrepo" style="display: flex; align-items: center; text-decoration: none; color: #000;">
      </a>
    </div>
  </div>
</div>

<div style="display: flex; align-items: center; margin-bottom: 40px;">
  <img src="images/GOV-NeSF.jpg" alt="GOV-NeSF: Generalizable Open-Vocabulary Neural Semantic Fields" style="width: 350px; height: auto; margin-right: 20px;">

  <div>
    <h3 style="margin: 0;">GOV-NeSF: Generalizable Open-Vocabulary Neural Semantic Fields</h3>
    <p style="margin: 5px 0;">
          <strong>Yunsong Wang</strong>,
          <a href="https://hlinchen.github.io/">Hanlin Chen</a>,
          <a href="https://www.comp.nus.edu.sg/~leegh/">Gim Hee Lee</a>
          <br>
      <b><em>CVPR 2024</em></b><br>
      <!-- color: #0073e6; -->
      <!-- <a href="https://projectpage.com" style="text-decoration: none; ">Project Page</a> / -->
      <a href="https://arxiv.org/pdf/2404.00931" style="text-decoration: none;">[PDF]</a> 
      <a href="https://github.com/wangys16/GOV-NeSF" style="text-decoration: none;">[Code]</a>
      <!-- <a href="https://video.com" style="text-decoration: none;">Video</a> / -->
      <!-- <a class="more-link" href="https://github.com/HeliosZhao/Animate124" target="_blank"><img alt="GitHub stars" align="right"
        src="https://img.shields.io/github/stars/HeliosZhao/Animate124?style=social"></a> -->
    </p>
    <!-- <p style="margin: 5px 0;">
      The first work to animate a single in-the-wild image into 3D video through textual motion descriptions.
    </p> -->
    <div style="display: flex; align-items: center; margin-top: 10px;">
      <a href="https://github.com/yourrepo" style="display: flex; align-items: center; text-decoration: none; color: #000;">
      </a>
    </div>
  </div>
</div>

<div style="display: flex; align-items: center; margin-bottom: 40px;">
  <img src="images/rethink.jpg" alt="Rethinking Visibility in Human Pose Estimation: Occluded Pose Reasoning via Transformers" style="width: 350px; height: auto; margin-right: 20px;">

  <div>
    <h3 style="margin: 0;">Rethinking Visibility in Human Pose Estimation: Occluded Pose Reasoning via Transformer</h3>
    <p style="margin: 5px 0;">
      <a href="https://pengzhansun.github.io/">Pengzhan Sun</a>,
      <a href="https://www.comp.nus.edu.sg/~keruigu/">Kerui Gu</a>,
      <strong>Yunsong Wang</strong>,
      <a href="=https://mu4yang.com">Linlin Yang</a>,
      <a href="https://www.comp.nus.edu.sg/~ayao/">Angela Yao</a>
      <br>
      <b><em>WACV 2024 (<i style="color:#e74d3c"> Oral Presentation </i>)</em></b><br>
      <a href="https://openaccess.thecvf.com/content/WACV2024/papers/Sun_Rethinking_Visibility_in_Human_Pose_Estimation_Occluded_Pose_Reasoning_via_WACV_2024_paper.pdf" style="text-decoration: none;">[PDF]</a>
      <a href="https://github.com/pengzhansun/Occluded-Pose-Reasoning" style="text-decoration: none;">[Code]</a>
    </p>
  </div>
</div>

<div style="display: flex; align-items: center; margin-bottom: 40px;">
  <img src="images/GRL.jpg" alt="Enhancing Generalizability of Representation Learning for Data-Efficient 3D Scene Understanding" style="width: 350px; height: auto; margin-right: 20px;">

  <div>
    <h3 style="margin: 0;">Enhancing Generalizability of Representation Learning for Data-Efficient 3D Scene Understanding</h3>
    <p style="margin: 5px 0;">
      <strong>Yunsong Wang</strong>,
      <a href="https://na-z.github.io">Na Zhao</a>,
      <a href="https://www.comp.nus.edu.sg/~leegh/">Gim Hee Lee</a>
      <br>
      <b><em>3DV 2024 (<i style="color:#e74d3c"> Oral Presentation </i>)</em></b><br>
      <a href="https://arxiv.org/abs/2406.11283" style="text-decoration: none;">[PDF]</a>
      <!--<a href="https://github.com/HLinChen/GNeSF" style="text-decoration: none;">[Code]</a>-->
    </p>
  </div>
</div>

<div style="display: flex; align-items: center; margin-bottom: 40px;">
  <img src="images/ISL.png" alt="Instance Similarity Learning for Unsupervised Feature Representation" style="width: 350px; height: auto; margin-right: 20px;">

  <div>
    <h3 style="margin: 0;">Instance Similarity Learning for Unsupervised Feature Representation</h3>
    <p style="margin: 5px 0;">
      <a href="https://ziweiwangthu.github.io">Ziwei Wang</a>,
      <strong>Yunsong Wang</strong>,
      <a href="https://wuziyi616.github.io">Ziyi Wu</a>,
      <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>,
      <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>
      <br>
      <b><em>ICCV 2021</em></b><br>
      <a href="https://arxiv.org/abs/2108.02721" style="text-decoration: none;">[PDF]</a>
      <a href="https://github.com/ZiweiWangTHU/ISL" style="text-decoration: none;">[Code]</a>
    </p>
  </div>
</div>

